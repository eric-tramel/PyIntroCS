{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity\n",
    "In this notebook we will take a look at sparse signals and note some of their properties. We will follow the following notational conventions. We will define a signal, $\\mathbf{x} \\in \\mathbb{R}^N$ to be *sparse* if $K$ elements of $\\mathbf{x}$ are non-zero, while the remaining elements are exactly zero. We will define the *sparsity* of $\\mathbf{x}$ to be the ratio of the number of non-zeros to zeros,\n",
    "$$\\rho = \\frac{K}{N}.$$\n",
    "\n",
    "Below, we will take a look at a few such signals and study their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Including Dependencies\n",
    "%matplotlib inline         \n",
    "import scipy\n",
    "import numpy as np                  # Numpy, très important\n",
    "import matplotlib.pyplot as plt     # PyPlot for Plotting\n",
    "import scipy.fftpack as fftpk       # FFTPack for Harmonic Analy.\n",
    "import scipy.linalg as la           # For Linear Algebra\n",
    "import pywt as pw                   # PyWavelets\n",
    "import scipy.signal as sig          # For signal processing, like conv\n",
    "from DepsSparsity import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random K-Sparse Signals\n",
    "---\n",
    "\n",
    "For now, let us confine ourselves to synthetic examples of exactly $K$-Sparse signals. Let us take a look at how to generate such a signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomKSparseSignal(K,N):\n",
    "    \"\"\" Random realization of a K-sparse signal.\n",
    "    \n",
    "        Assumes that all non-zero components of the realization\n",
    "        are drawn iid from a zero-mean Gaussian with a variance \n",
    "        of ``1``. \n",
    "    \n",
    "        Input\n",
    "        ---\n",
    "        K: int\n",
    "           Number of non-zeros in the signal.\n",
    "        N: int\n",
    "           Signal dimensionality (length)\n",
    "        \n",
    "        Returns\n",
    "        ---\n",
    "        x: array_like, float\n",
    "           The K-sparse signal realization, shaped ``(n,1)``\n",
    "        s: array_like, bool\n",
    "           The support of ``x``, shaped ``(n,1)``. Has value\n",
    "           ``True`` at every non-zero location of ``x`` and\n",
    "           ``False`` everywhere else.\n",
    "           \n",
    "        \n",
    "    \"\"\"\n",
    "    x = np.random.randn(N,1)                # Generate iid Gaussian samples     \n",
    "    s = np.zeros((N,1),dtype=bool)          # Cast ``0.0`` as ``False``\n",
    "    si = np.random.permutation(N)[1:K]      # Random support locations\n",
    "    s[si] = True                            # Flag on-support values\n",
    "    x[np.invert(s)] = 0.0                   # Suppress off-support values of ``x``\n",
    "    \n",
    "    return x, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take a look at what what one of these $K$-sparse signals looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1024         # Desired signal dimensionality\n",
    "K = 64           # Number of non-zeros\n",
    "xsparse,xsparse_support = RandomKSparseSignal(K,N)\n",
    "ShowSignal(xsparse);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the signal is mostly a constant value of `0` with a $K$ non-zero values randomly distributed amongst it. Often in practice, it is not quite so clear that our signal of interest is $K$-sparse or even compressible. So, another way of looking at the signal is by its sorted coefficient magnitudes, or its *deacay*. Specifically, lets call the signal $\\tilde{x}$ the *sorted* veresion of $x$. Then, we have\n",
    "$$|\\tilde{x}_1| \\leq |\\tilde{x}_2| \\leq \\cdots \\leq |\\tilde{x}_N|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowDecay(xsparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture that we defined a *compressible* signal as one which follows a *power-law decay*. Specifically, if we sort $x$ by the magnitude of its coefficients and call this signal $\\tilde{x}$, the original signal $x$ is said to obey a power-law decay if\n",
    "\n",
    "$$\\left|\\tilde{x}_s\\right| \\leq C s^{-\\alpha} \\quad \\text{for} \\quad s = 1,2,\\cdots$$\n",
    "\n",
    "Can we say that this $K$-sparse signal obeys a strong power-law decay? Is it essentially *compressible*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowDecay(xsparse,scalemode='log');\n",
    "## Can we find the proper set of parameters \n",
    "## which bound the decay?\n",
    "C = 5.5         \n",
    "a = 0.5\n",
    "s = np.arange(1,N)\n",
    "plt.plot(s,C*np.power(s,-a),linewidth=2);\n",
    "## Uncomment these lines to see the decay comparison\n",
    "## in the linear domain.\n",
    "# plt.xscale('linear')\n",
    "# plt.yscale('linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets compare our $K$-Sparse signal to one which is merely compressible, but not truly sparce. In this case, lets look at an \n",
    "*iid* Laplacian signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomLaplaceSignal(scale,N):\n",
    "    \"\"\" Random realization of an iid Laplace signal. \n",
    "    \n",
    "        Input\n",
    "        ---\n",
    "        K: float\n",
    "           Scale parameter for the Laplacian distribution.\n",
    "        N: int\n",
    "           Signal dimensionality (length)\n",
    "        \n",
    "        Returns\n",
    "        ---\n",
    "        x: array_like, float\n",
    "           The iid Laplace signal realization, shaped ``(n,1)``                \n",
    "    \"\"\"\n",
    "    x = np.random.laplace(0.0,scale=scale,size=(N,1))    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xlaplace = RandomLaplaceSignal(np.sqrt(0.25),N)\n",
    "ShowSignal(xlaplace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, we can see that this Laplacian *iid* signal has values which are concentrated around 0, but with a few large outliers. Again, lets take a look at the decay profile of signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ShowDecay(xlaplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, likewise, we can take a look at the power-law decay which this signal obeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowDecay(xlaplace,scalemode='log');\n",
    "## Can we find the proper set of parameters \n",
    "## which bound the decay?\n",
    "C = 4       \n",
    "a = 0.25\n",
    "s = np.arange(1,N)\n",
    "plt.plot(s,C*np.power(s,-a),linewidth=2);\n",
    "## Uncomment these lines to see the decay comparison\n",
    "## in the linear domain.\n",
    "# plt.xscale('linear')\n",
    "# plt.yscale('linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we compare the two signals, and from our previous power-law decay fitting, we can see that the $K$-sparse signal is indeed more compressible than a Laplacian signal, one which is by its essence \"compressible\". Below, we can see the direct comparison between the two decays of the two different kinds of signals, the $K$-Sparse signal, and the compressible one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CompareDecayTwo('K-Sparse',xsparse,'Laplacian',xlaplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stark difference is also apaprent when we look at the distribution of coefficient magnitudes in the following histogram. We belabor this point so that we can get a feel for the what it is to observe a truly \"sparse\" signal in the wild, as compared to a \"compressible\" one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CompareHistograms('K-Sparse',xsparse,'Laplacian',xlaplace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the nice properties of compressible signals, however, is that they are never too far away from a $K$-sparse represenation. Because the power-law property of compressible signals forces the majority of information to be concentrated on a few coefficients, by retaining only the top $K$ coefficients, we are able to create a $K$-sparse approximation of a compressible signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def KSparseApproximation(x,K):\n",
    "    \"\"\" Retain the top K coefficients in  x.\n",
    "    \n",
    "    Given a signal, find the top K-magnitude coefficients.\n",
    "    Return a signal which retains these identified coefficients\n",
    "    and sets the rest to 0.\n",
    "    \n",
    "    Input\n",
    "    ---\n",
    "    x: array_like, float\n",
    "       The signal that we wish to approximate\n",
    "       \n",
    "    K: int\n",
    "       Number of coefficients to retain.\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    xT: array_like, float\n",
    "        The K-Sparse approximation of x\n",
    "    S:  array_like, int\n",
    "        An array of the support index locations in x.    \n",
    "    \"\"\"\n",
    "    S = np.argsort(abs(x),axis=0)[::-1][:K]\n",
    "    xT = np.zeros_like(x)\n",
    "    xT[S] = x[S]\n",
    "    \n",
    "    return xT, S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "KApprox = 50    # Number of non-zeros to retain\n",
    "xlaplace_approx = KSparseApproximation(xlaplace,KApprox)[0]\n",
    "\n",
    "ShowSignal(xlaplace)\n",
    "plt.title(\"Original Laplacian Signal\",fontsize=20)\n",
    "ShowSignal(xlaplace_approx)\n",
    "plt.title(\"$K$-Sparse Approximation\",fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making an approximation, we should have a way of measuring the quality of that approximation. Below we define a number of metrics that we can use to compare two signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MeanSquareError(x,y):\n",
    "    \"\"\" Compute MSE between two signals.\n",
    "    \n",
    "    Returns the mean square error, which \n",
    "    is calculated by taking the squared \n",
    "    $\\ell_2$ norm of the distortion between\n",
    "    ``x`` and ``y`` and dividing by the \n",
    "    signal length.\n",
    "    \n",
    "    Inputs\n",
    "    ---\n",
    "    x: array_like, float\n",
    "       A signal.\n",
    "    y: array_like, float\n",
    "       A signal.\n",
    "       \n",
    "    Returns\n",
    "    ---\n",
    "    mseval: float\n",
    "            The Mean Square Error value.\n",
    "    \n",
    "    \"\"\"\n",
    "    mseval = np.mean(np.power(x-y,2))    \n",
    "    return mseval\n",
    "\n",
    "def Correlation(x,y):\n",
    "    \"\"\" Compute the correlation coefficient between\n",
    "        two signals.\n",
    "        \n",
    "        Inputs\n",
    "        ---\n",
    "        x: array_like, float\n",
    "           A signal.\n",
    "        y: array_like, float\n",
    "           A signal.\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        corrcoef: float\n",
    "                  The correlation between the two vectors.\n",
    "    \"\"\"\n",
    "    n = np.size(x)\n",
    "    x = np.reshape(x,(1,n))\n",
    "    y = np.reshape(y,(1,n))\n",
    "    return np.corrcoef(x,y)[1,0]\n",
    "\n",
    "def SignalToNoise(x,y):\n",
    "    \"\"\" Compute the SNR between ``x``, the original signal,\n",
    "        and the noisy version, ``y``.\n",
    "\n",
    "        Inputs\n",
    "        ---\n",
    "        x: array_like, float\n",
    "           The original signal.\n",
    "        y: array_like, float\n",
    "           The corrupted signal\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        snr: float\n",
    "             The signal-to-noise ratio in decibels.\n",
    "    \"\"\"\n",
    "    Noise = y - x\n",
    "    return 10*np.log10(np.var(x)/np.var(Noise))\n",
    "    \n",
    "def EnergyRatio(x,y):\n",
    "    \"\"\" Compute the energy (squared $\\ell_2$ norm) ratio\n",
    "        between two signals.\n",
    "\n",
    "        Inputs\n",
    "        ---\n",
    "        x: array_like, float\n",
    "           A signal.\n",
    "        y: array_like, float\n",
    "           A signal.\n",
    "\n",
    "        Returns\n",
    "        ---\n",
    "        er: float\n",
    "            The ratio of the signal ``x`` energy to that of \n",
    "            signal ``y``.\n",
    "    \"\"\"\n",
    "    n = np.size(x)\n",
    "    x = np.reshape(x,(n,1))\n",
    "    y = np.reshape(y,(n,1))\n",
    "    \n",
    "    xeng = np.linalg.norm(x,2)\n",
    "    xeng = xeng*xeng\n",
    "    \n",
    "    yeng = np.linalg.norm(y,2)\n",
    "    yeng = yeng*yeng\n",
    "    \n",
    "    return xeng/yeng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We below, we can see how well our approximation from above has done in terms of the metrics we've defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(MeanSquareError(xlaplace,xlaplace_approx))\n",
    "print(Correlation(xlaplace,xlaplace_approx))    # Between [-1,1]\n",
    "print(SignalToNoise(xlaplace,xlaplace_approx))  # Measured in dB\n",
    "print(EnergyRatio(xlaplace_approx,xlaplace))    # Value > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of particular note is the last term, the energy ratio between the approximation and the orignal signal. We term this measure the *energy-loss* of the approximation. It can be thought of as a measure of how much information about the original signal we lost through the process of our approximation.\n",
    "\n",
    "To illustrate the point that compressible signals are amenable to $K$-sparse representation, but that not all signals are, lets compare our compressible Laplacian signal to a realization of an *iid* Gaussian signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RandomGaussSignal(sig,N):\n",
    "    \"\"\" Random realization of an iid Gaussian signal. \n",
    "    \n",
    "        Input\n",
    "        ---\n",
    "        sig: float\n",
    "             Standard deviation of the generating Normal\n",
    "             distribution.\n",
    "        N: int\n",
    "           Signal dimensionality (length)\n",
    "        \n",
    "        Returns\n",
    "        ---\n",
    "        x: array_like, float\n",
    "           The iid Gaussian signal realization, shaped ``(n,1)``                \n",
    "    \"\"\"\n",
    "    x = sig*np.random.randn(N,1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgauss = RandomGaussSignal(1.0,N)\n",
    "ShowSignal(xgauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this signal compressible? It is the essense of high-entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowDecay(xgauss,scalemode='log');\n",
    "## Can we find the proper set of parameters \n",
    "## which bound the decay?\n",
    "C = 4       \n",
    "a = 0.1\n",
    "s = np.arange(1,N)\n",
    "plt.plot(s,C*np.power(s,-a),linewidth=2);\n",
    "## Uncomment these lines to see the decay comparison\n",
    "## in the linear domain.\n",
    "# plt.xscale('linear')\n",
    "# plt.yscale('linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above comparison to the fitted power-law decay, we see that the Gaussian signal does not exhibit the properties of a power-law decay. To give a comparison, what happens if we take take a $K$-sparse approximation of this Gaussian signal, keeping the same number of non-zeros as in the case of the Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xgauss_approx = KSparseApproximation(xgauss,KApprox)[0]\n",
    "\n",
    "ShowSignal(xgauss)\n",
    "plt.title(\"Original Gaussian Signal\",fontsize=20)\n",
    "ShowSignal(xgauss_approx)\n",
    "plt.title(\"$K$-Sparse Approximation\",fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, using the same distortion measures as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(MeanSquareError(xgauss,xgauss_approx))\n",
    "print(Correlation(xgauss,xgauss_approx))    # Between [-1,1]\n",
    "print(SignalToNoise(xgauss,xgauss_approx))  # Measured in dB\n",
    "print(EnergyRatio(xgauss_approx,xgauss))    # Value > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that since the Gaussian signal has its magnitudes more evenly distributed, less energy/information is stored within the $K$ largest coefficients, and the $K$-sparse approximation of the Gaussian signal is not as close as is the case for the compressible Laplacian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gauss_ER_TestedK  = np.zeros((N-1,1))\n",
    "laplace_ER_TestedK  = np.zeros((N-1,1))\n",
    "\n",
    "gauss_SNR_TestedK  = np.zeros((N-1,1))\n",
    "laplace_SNR_TestedK  = np.zeros((N-1,1))\n",
    "\n",
    "# Loop over all the K-Sparse approximations\n",
    "TestedK = np.arange(1,N)\n",
    "for k in TestedK:\n",
    "    laplaceapprox_k = KSparseApproximation(xlaplace,k)[0]\n",
    "    gaussapprox_k = KSparseApproximation(xgauss,k)[0]\n",
    "    \n",
    "    laplace_ER_TestedK[k-1] = EnergyRatio(laplaceapprox_k,xlaplace)\n",
    "    gauss_ER_TestedK[k-1] = EnergyRatio(gaussapprox_k,xgauss)\n",
    "\n",
    "    \n",
    "CompareER('Laplace',laplace_ER_TestedK,'Gaussian',gauss_ER_TestedK,TestedK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here, we can see very easily that compressible signals are much more amenable to $K$-sparse approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sparsity in Other Bases\n",
    "It is a rare case that we find a signal of interest which is sparse in the direct (ambient) domain. Of course, such examples exist, for example, PALM and STORM acquired images for fluorescence microscopy can be seen to be sparse in direct space, as well as some astronomical images.\n",
    "\n",
    "Most often, we have a signal which is essential non-sparse, but does contain some pattern or regulariaty. Finding a *sparse basis* often amounts to exploiting the frequency content of the signal (i.e. harmonic analysis) to find a compact representation of the information contained in the signal. \n",
    "\n",
    "Lets take a look at an example from image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage as img      # An image processing library\n",
    "\n",
    "## Load the test image\n",
    "TestImage = img.imread(\"elephants.jpg\")\n",
    "plt.imshow(TestImage);\n",
    "plt.title(\"A Test Image\",fontsize=18);\n",
    "\n",
    "## Converting the image to grayscale, floating point, and normalized\n",
    "X = np.array(TestImage,dtype=float)\n",
    "X = X[:,:,0]/3 + X[:,:,1]/3 + X[:,:,2]/3\n",
    "X = X / la.norm(X)\n",
    "\n",
    "plt.matshow(X);\n",
    "plt.gray();\n",
    "plt.title(\"Grayscale Version\",fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here we have a nice test image. Is this image sparse? Is it compressible? It is not immediately clear from looking at it in the image domain. However, intuitively, we can see that there should be a concise description of this image. Very broadly, the image consists of just 3 different textures, the sky, the ground, and the elephants. Within these textures the textures are more or less consistent. So, the information cotent of the image, beyond describing these particular textures, is the description of the boundaries, or edges, between the textures.\n",
    "\n",
    "Note that, above, we have converted the image to grayscale. This makes the example simpler. Otherwise, we would need to consider three different color channels in all of our operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Cosine Transform\n",
    "---\n",
    "\n",
    "One of the commonly used transforms in image compression (ala the JPEG compression standard) is the *Discrete Cosine Transform*. One can think of the DCT as a real version of the Fourier Transform. The DCT comes in a few different flavors, but \"the\" DCT is known as the Type 2 DCT, or DCT-II. The DCT-II of a signal is given as\n",
    "\n",
    "$$X_j = \\sum_{i=0}^{N-1} x_i \\cos \\left[\\frac{\\pi}{N}(i+0.5) j\\right],$$\n",
    "\n",
    "where the coefficients $X_j$ refer to our DCT-II coefficients. Like the Fourier transform, the DCT-II builds up an exact representation of $x$ via waveforms of varying frequency, here, cosines. Also like the Fourier transform, we can see that the DCT-II is a *linear* transform, that is, all coefficients are defined according to a sum over the time-domain coefficients and vice-versa. Finally, the DCT-II is also an *orthonormal* transform, so its transpose serves as its own inverse,\n",
    "\n",
    "$$\\Psi_{\\text DCT}^T \\Psi_{\\text DCT} = I$$.\n",
    "\n",
    "Lets take a look at the DCT-II coefficients of this test image. This requires us to apply the DCT-II in both the horizontal and vertical directions (the same procedure which is used to compute the 2D FFT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xdct = fftpk.dct(fftpk.dct(X.T).T)  # Apply to both directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Out of curiosity, what do these coefficients look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(Xdct);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The scale of the values of the DCT-II is very widely varying, so to *see* the coefficients visually, we usually plot the magnitudes on a log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(np.log10(np.abs(Xdct)));\n",
    "plt.jet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cursorially, we see that the largest magnitudes are bunching towards the top-left corner of the 2D set of DCT-II coefficients. This region corresponds to the lowest-frequency terms in our DCT-II transform space. The implication is that the low-pass portion of the DCT-II space contains the majority of the information about the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(np.log10(np.abs(Xdct[0:50,0:50])));\n",
    "plt.jet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even zooming further onto the low-pass coefficeints, we see that the information is further clustered into the lower $10\\times 7$ set of frequencies. What happens if we retain these but none of the others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xdct_lopass = np.zeros(np.shape(Xdct))\n",
    "Xdct_lopass[0:6,0:9] = Xdct[0:6,0:9]\n",
    "Xrec =  fftpk.idct(fftpk.idct(Xdct_lopass.T,norm='ortho').T,norm='ortho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(Xrec)\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can see that these coefficeints contain information about the darker regions (the elephants), the ground, and the sky. However, this is not enough information to be useful. What about if we keep the $50\\times 50$ region?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xdct_lopass = np.zeros(np.shape(Xdct),dtype=float)\n",
    "Xdct_lopass[0:49,0:49] = Xdct[0:49,0:49]\n",
    "# Xdct_lopass = np.array(Xdct)\n",
    "plt.matshow(np.log10(np.abs(Xdct_lopass)))\n",
    "plt.jet()\n",
    "\n",
    "Xrec =  fftpk.idct(fftpk.idct(Xdct_lopass.T,norm='ortho').T,norm='ortho')\n",
    "plt.matshow(Xrec)\n",
    "plt.gray();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a recognizable image. How many coefficeints did we retain, here? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"%f%% Coefficients Retained\" % (100*(50.0*50.0)/np.size(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even with a very small percentage of the coefficients, we still retain much of the information. The image is still recognizable as three elephants in a line on a field with a horizon in the background. Adding more frequency components will allow us to get closer and closer to the original image. \n",
    "\n",
    "Just how compressible is this image? We can look at the weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowDecay(Xdct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it fit a power-law?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowDecay(Xdct,scalemode='log');\n",
    "## Can we find the proper set of parameters \n",
    "## which bound the decay?\n",
    "C = 1100       \n",
    "a = 0.9\n",
    "s = np.arange(1,np.size(X))\n",
    "plt.plot(s,C*np.power(s,-a),linewidth=2);\n",
    "## Uncomment these lines to see the decay comparison\n",
    "## in the linear domain.\n",
    "# plt.xscale('linear')\n",
    "# plt.yscale('linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, in fact, we see that it does indeed obey a power-law decay quite well, hence the use of DCT-II for $K$-sparse representation of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet Transform\n",
    "---\n",
    "Yet another tool which is used for image compression are *wavelets* (ala JPEG2000). The study of wavelets is a huge topic and is somewhat beyond the scope of this notebook. The best way to think about wavelets is that this representation exists half way between the time- and frequency-basis. Recall that when we use a harmonic analysis such as the Fourier transform or the DCT, we seek to represent our signal as a superposition of many periodic (*infinitely long*) sinusoids. These sinusoids are completely non-local. As such, a single sinusoid will tell us some information about every time sample, and vice-versa. \n",
    "\n",
    "In the case of wavelets, (né «onde-lettes»), a signal is represented by *localized* sinusoids. These localized shapes can exist at any location **and** at any scale. Because of this, wavlet representations of a signal are naturally *multi-scale*, capturing the structure of the signal across many different levels of detail. \n",
    "\n",
    "One can represent a signal *exactly* with a wavelet representation, just as is the case with the FFT and DCT-II, and this is accomplished by clever self-referential properties of the wavelet construction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Levels = 5      # Number of levels of wavelet decomposition\n",
    "W = pw.wavedec2(X,'db1',level=Levels,mode='sym');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets take a look at the wavelet coefficients. Generally, this is done by arranging the coefficients in the same spatial orientation as the original image, concentrating the lower-frequency information towards the top left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ShowWaveletCoeffs(W,Magnitude=False,cmapname='jet');\n",
    "plt.title(\"Wavelet Coefficients\",fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we suppress the baseband and measure the *magnitude* of the wavelet coefficients so we can get a better view of detail (non-baseband) coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowWaveletCoeffs(W,SuppressBaseband=True,Magnitude=True,cmapname='PuRd')\n",
    "plt.title(\"Wavelet Coefficients (sans Baseband)\",fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the extreme left-hand corner we have the coefficients which are known as the *baseband*. These coefficients correspond to the idea of the DC coeffcient of a frequency transform such as the FFT or DCT. An image is *decomposed* with wavlets in a layer-by-layer fashion. At each layer, three channels of coefficients, **horizontal**, **vertical**, and **vertical**, are calculated, along with a baseband. Each successive level of wavelet decomposition performs the same calculation on the baseband of the previous level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that the wavelet coefficients are themselves quite compressible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowWaveletDecay(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see how well we can approximate this image using a $K$ sparse representation of the wavelet coefficients. As comparison, lets take the same $K$ value as our earlier DCT experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = 50*50     # Match the same number of coeffs as DCT\n",
    "WK = WaveletTopKApproximation(W,K)\n",
    "print \"%f%% Coefficients Retained\" % (100.0*(float(K))/np.size(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we view the thresholded wavelet coefficients. Unfortunately, the scale of the plot generated by `PyPlot` doesn't give us a very good view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ShowWaveletCoeffs(WK,SuppressBaseband=False,Magnitude=False,cmapname='jet');\n",
    "plt.title(\"Thresholded Wavelet Coefficients\",fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets see what the recovered spatial-domain image looks like when we drop so many wavelet coefficients from the representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "XRec = pw.waverec2(WK,'db1')\n",
    "\n",
    "plt.matshow(XRec);\n",
    "plt.gray();\n",
    "plt.title(\"Recovered From Few Wavelet Coeffs\",fontsize=18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
